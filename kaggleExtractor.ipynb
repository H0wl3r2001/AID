{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle dataset extractor and organizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = 'C:/Users/afons/.kaggle/'\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove feather files, if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feather files removed from the 'data' folder.\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'data'\n",
    "\n",
    "# List all files in the data folder\n",
    "files = os.listdir(data_folder)\n",
    "\n",
    "# Remove Feather files (files with a \".feather\" extension)\n",
    "for file in files:\n",
    "    if file.endswith(\".feather\"):\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed {file_path}\")\n",
    "\n",
    "print(\"Feather files removed from the 'data' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the dataset on Kaggle\n",
    "dataset_path = 'sumaiaparveenshupti/los-angeles-crime-data-20102020'\n",
    "\n",
    "# download the dataset to the data folder\n",
    "kaggle.api.dataset_download_files(dataset_path, path='data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the files from zip, get into feather files and remove csv and zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the path to the zip file\n",
    "zip_path = 'data/los-angeles-crime-data-20102020.zip'\n",
    "\n",
    "# extract all csv files from the zip file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned into 10 smaller CSV files in the \"data\" folder.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Create the \"data\" directory if it doesn't exist\n",
    "data_dir = 'data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Specify the input CSV file and the number of partitions\n",
    "input_file = 'data/Crime_Data_from_2010_to_2019.csv'\n",
    "num_partitions = 10  # Change this to the desired number of partitions\n",
    "\n",
    "# Initialize a list of output CSV writers and files\n",
    "output_writers = []\n",
    "output_files = []\n",
    "\n",
    "# Open the input file\n",
    "with open(input_file, 'r') as input_csv:\n",
    "    # Create a CSV reader for the input file\n",
    "    csv_reader = csv.reader(input_csv)\n",
    "\n",
    "    # Read the CSV header\n",
    "    header = next(csv_reader)\n",
    "\n",
    "    # Open output CSV files and writers for each partition in the \"data\" directory\n",
    "    for i in range(num_partitions):\n",
    "        partition_file = os.path.join(data_dir, f'Crime_Data_from_2010_to_2019_{i}.csv')\n",
    "        output_file = open(partition_file, 'w', newline='')\n",
    "        output_files.append(output_file)\n",
    "        output_writers.append(csv.writer(output_file))\n",
    "\n",
    "        # Write the header to each output file\n",
    "        output_writers[i].writerow(header)\n",
    "\n",
    "    # Iterate through the input CSV and distribute rows to output partitions\n",
    "    current_partition = 0\n",
    "    for row in csv_reader:\n",
    "        output_writers[current_partition].writerow(row)\n",
    "        current_partition = (current_partition + 1) % num_partitions  # Cycle through partitions\n",
    "\n",
    "# Close all output CSV files\n",
    "for output_file in output_files:\n",
    "    output_file.close()\n",
    "\n",
    "print(f'Partitioned into {num_partitions} smaller CSV files in the \"data\" folder.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Crime_Data_from_2010_to_2019.csv...\n",
      "Processing Crime_Data_from_2010_to_2019_0.csv...\n",
      "Processing Crime_Data_from_2010_to_2019_1.csv...\n",
      "Processing Crime_Data_from_2010_to_2019_2.csv...\n",
      "Processing Crime_Data_from_2010_to_2019_3.csv...\n",
      "Processing Crime_Data_from_2010_to_2019_4.csv...\n",
      "Processing Crime_Data_from_2010_to_2019_5.csv...\n",
      "Processing Crime_Data_from_2010_to_2019_6.csv...\n",
      "Processing Crime_Data_from_2010_to_2019_7.csv...\n",
      "Processing Crime_Data_from_2010_to_2019_8.csv...\n",
      "Processing Crime_Data_from_2010_to_2019_9.csv...\n",
      "Processing Crime_Data_from_2020_to_Present.csv...\n"
     ]
    }
   ],
   "source": [
    "# iterate over all csv files in the data folder\n",
    "for file_name in os.listdir('data'):\n",
    "    if file_name.endswith('.csv'):\n",
    "        print(f'Processing {file_name}...')\n",
    "        # read the csv file into a pandas dataframe\n",
    "        df = pd.read_csv(os.path.join('data', file_name))\n",
    "        \n",
    "        # transform the dataframe into more space-efficient datatypes\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'int64':\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "            elif df[col].dtype == 'float64':\n",
    "                df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        \n",
    "        # save the transformed dataframe as a feather file\n",
    "        feather_path = os.path.join('data', os.path.splitext(file_name)[0] + '.feather')\n",
    "        df.to_feather(feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the zip file and the csv files\n",
    "os.remove('data/los-angeles-crime-data-20102020.zip')\n",
    "for file_name in os.listdir('data'):\n",
    "    if file_name.endswith('.csv'):\n",
    "        os.remove(os.path.join('data', file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder size is: 465.43 MB\n"
     ]
    }
   ],
   "source": [
    "def get_folder_size(path):\n",
    "    total_size = 0\n",
    "\n",
    "    # Walk through the directory tree and add up the sizes of all files and subdirectories\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# Specify the path to the folder you want to measure\n",
    "folder_path = 'data'\n",
    "\n",
    "# Get the folder size in bytes\n",
    "size_in_bytes = get_folder_size(folder_path)\n",
    "\n",
    "# Convert the size to a more human-readable format (e.g., MB, GB)\n",
    "def convert_bytes_to_readable(size_in_bytes):\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_in_bytes < 1024.0:\n",
    "            break\n",
    "        size_in_bytes /= 1024.0\n",
    "    return f\"{size_in_bytes:.2f} {unit}\"\n",
    "\n",
    "folder_size_readable = convert_bytes_to_readable(size_in_bytes)\n",
    "\n",
    "print(f\"The folder size is: {folder_size_readable}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
